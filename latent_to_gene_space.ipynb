{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from model_VAE import VAE\n",
    "from dataloader_VAE import get_h5ad_data, get_dataloader, normalize, inverse_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'DR'\n",
    "\n",
    "\n",
    "para_dict = {\n",
    "    'DR': 'drosophila_scNODE2_2000genes_2489cells_11tps.h5ad',\n",
    "    'EB': 'embryoid_scNODE5_2000genes_6232cells_5tps.h5ad',\n",
    "    'MB': 'mammalian_scNODE1_2000genes_7542cells_13tps.h5ad',\n",
    "    'MP': 'pancreatic_scNODE4_2000genes_9483cells_4tps.h5ad',\n",
    "    'ZB': 'zebrafish_scNODE0_2000genes_3227cells_12tps.h5ad',\n",
    "}\n",
    "\n",
    "\n",
    "dataset_h5ad = para_dict[dataset_name]\n",
    "\n",
    "# result_path = f'/home/hanyuji/Results/scDYff/interpolation_latent/{dataset_name}_result_dict_50_latent.pt'\n",
    "result_path = f'/home/hanyuji/Results/scDYff/interpolation_latent/{dataset_name}_result_dict_50_latent_3000cell.pt'\n",
    "with open(result_path, 'rb') as f:\n",
    "    latent_data_dict = pickle.load(f)\n",
    "\n",
    "if dataset_name in ['MP', 'EB']:\n",
    "    test_index = [\n",
    "        2,\n",
    "    ]\n",
    "else:\n",
    "    test_index = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "\n",
    "# 训练集和测试集对应的下标\n",
    "test_list = [latent_data_dict[i] for i in test_index]\n",
    "test_dataloader = get_dataloader(test_list, test_index, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据缩放因子\n",
    "data_list = get_h5ad_data(dataset_h5ad)\n",
    "\n",
    "norm_data_list, scalers = normalize(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "device = \"cuda:0\"\n",
    "net = VAE().to(device)\n",
    "model_path = f'/home/hanyuji/Results/VAE_result/model_para/vae_model_0604_{dataset_name}_all.pt'\n",
    "net.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_dict = {}  # 12个数组\n",
    "\n",
    "# {4: [], 6: [], 8: []}\n",
    "for index in test_index:\n",
    "    recon_dict[index] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "for (x, y) in test_dataloader:\n",
    "    \n",
    "    x = x.float().to(device)\n",
    "    recon = net.decoder(x)\n",
    "    \n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    recon_np = recon.detach().cpu().numpy()\n",
    "                \n",
    "    for (recon_i,y_i) in zip(recon_np,y_np):\n",
    "        recon_dict[y_i].append(recon_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_list = []\n",
    "    \n",
    "for index, arr in recon_dict.items():\n",
    "    recon_list.append(np.asarray(arr))\n",
    "\n",
    "inverse_norm_recon_list = inverse_normalize(recon_list, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存这些数组到一个文件\n",
    "\n",
    "file_name = f'{dataset_name}_2000_recon_3000cell.pkl'\n",
    "with open('/home/hanyuji/Results/VAE_result/data_recon_0606/'+file_name, 'wb') as f:\n",
    "    pickle.dump(inverse_norm_recon_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB scDYff_DiT\n",
      "ot: 109.12289428710938, l2: 18.197466965094875, cos: 0.2345526444695823, corr: 0.2558484355573812\n",
      "ot: 105.7564697265625, l2: 18.921055496911304, cos: 0.24143947949675806, corr: 0.26514125322703386\n",
      "ot: 116.9522705078125, l2: 16.774277772741225, cos: 0.1932824699523682, corr: 0.20929105330507125\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import geomloss\n",
    "\n",
    "print(dataset_name,'scDYff_DiT')\n",
    "\n",
    "for index_recon, index_true in zip(range(len(test_index)),test_index):\n",
    "    # 评估结果\n",
    "    x_pred = inverse_norm_recon_list[index_recon]\n",
    "    # x_pred = data_list[index_true-1]  # naive method\n",
    "    \n",
    "    x_true = data_list[index_true]\n",
    "\n",
    "    l2_dist = cdist(x_true, x_pred, metric=\"euclidean\")\n",
    "    cos_dist = cdist(x_true, x_pred, metric=\"cosine\")\n",
    "    corr_dist = cdist(x_true, x_pred, metric=\"correlation\")\n",
    "    avg_l2 = l2_dist.sum() / np.prod(l2_dist.shape)\n",
    "    avg_cos = cos_dist.sum() / np.prod(cos_dist.shape)\n",
    "    avg_corr = corr_dist.sum() / np.prod(corr_dist.shape)\n",
    "\n",
    "\n",
    "    ot_solver = geomloss.SamplesLoss(\n",
    "        \"sinkhorn\", p=2, blur=0.05, scaling=0.5, debias=True, backend=\"tensorized\"\n",
    "    )\n",
    "    ot = ot_solver(\n",
    "        torch.tensor(x_pred).type(torch.float32).to(device),\n",
    "        torch.tensor(x_true).type(torch.float32).to(device),\n",
    "    ).item()\n",
    "    # l2 = nn.MSELoss(x_pred, x_true)\n",
    "\n",
    "    print(f'ot: {ot}, l2: {avg_l2}, cos: {avg_cos}, corr: {avg_corr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_true = 4\n",
    "\n",
    "# batch_size = 2000\n",
    "\n",
    "# x1 = data_list[index_true - 1]\n",
    "# x3 = data_list[index_true + 1]\n",
    "# cell_idx_1 = np.random.choice(\n",
    "#     np.arange(x1.shape[0]), size=batch_size, replace=(x1.shape[0] < batch_size)\n",
    "# )\n",
    "# cell_idx_3 = np.random.choice(\n",
    "#     np.arange(x3.shape[0]), size=batch_size, replace=(x3.shape[0] < batch_size)\n",
    "# )\n",
    "# x1 = x1[cell_idx_1, :]\n",
    "# x3 = x3[cell_idx_3, :]\n",
    "\n",
    "# x_pred = (x1+x3)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB Linear Method\n",
      "ot: 146.31344604492188, l2: 20.65794167020932, cos: 0.28849095546102554, corr: 0.3142006418408773\n",
      "ot: 142.38583374023438, l2: 20.88380599968904, cos: 0.2828053162285965, corr: 0.30938833702307816\n",
      "ot: 158.1946563720703, l2: 20.40762634291961, cos: 0.2699596243096057, corr: 0.29267287460356545\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "import geomloss\n",
    "\n",
    "print(dataset_name, 'Linear Method')\n",
    "\n",
    "for index_recon, index_true in zip(range(len(test_index)),test_index):\n",
    "    # 评估结果\n",
    "    # x_pred = inverse_norm_recon_list[index_recon]\n",
    "\n",
    "    \n",
    "    \n",
    "    batch_size = 2000\n",
    "\n",
    "    x1 = data_list[index_true - 1]\n",
    "    x3 = data_list[index_true + 1]\n",
    "    cell_idx_1 = np.random.choice(\n",
    "        np.arange(x1.shape[0]), size=batch_size, replace=(x1.shape[0] < batch_size)\n",
    "    )\n",
    "    cell_idx_3 = np.random.choice(\n",
    "        np.arange(x3.shape[0]), size=batch_size, replace=(x3.shape[0] < batch_size)\n",
    "    )\n",
    "    x1 = x1[cell_idx_1, :]\n",
    "    x3 = x3[cell_idx_3, :]\n",
    "\n",
    "    x_pred = (x1+x3)/2\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x_true = data_list[index_true]\n",
    "\n",
    "    l2_dist = cdist(x_true, x_pred, metric=\"euclidean\")\n",
    "    cos_dist = cdist(x_true, x_pred, metric=\"cosine\")\n",
    "    corr_dist = cdist(x_true, x_pred, metric=\"correlation\")\n",
    "    avg_l2 = l2_dist.sum() / np.prod(l2_dist.shape)\n",
    "    avg_cos = cos_dist.sum() / np.prod(cos_dist.shape)\n",
    "    avg_corr = corr_dist.sum() / np.prod(corr_dist.shape)\n",
    "\n",
    "\n",
    "    ot_solver = geomloss.SamplesLoss(\n",
    "        \"sinkhorn\", p=2, blur=0.05, scaling=0.5, debias=True, backend=\"tensorized\"\n",
    "    )\n",
    "    ot = ot_solver(\n",
    "        torch.tensor(x_pred).type(torch.float32).to(device),\n",
    "        torch.tensor(x_true).type(torch.float32).to(device),\n",
    "    ).item()\n",
    "    # l2 = nn.MSELoss(x_pred, x_true)\n",
    "\n",
    "    print(f'ot: {ot}, l2: {avg_l2}, cos: {avg_cos}, corr: {avg_corr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial.distance import cdist\n",
    "# import geomloss\n",
    "\n",
    "# print(dataset_name, 'Naive Method')\n",
    "\n",
    "# for index_recon, index_true in zip(range(len(test_index)),test_index):\n",
    "#     # 评估结果\n",
    "#     # x_pred = inverse_norm_recon_list[index_recon]\n",
    "#     x_pred = data_list[index_true-1]  # naive method\n",
    "    \n",
    "#     x_true = data_list[index_true]\n",
    "\n",
    "#     l2_dist = cdist(x_true, x_pred, metric=\"euclidean\")\n",
    "#     cos_dist = cdist(x_true, x_pred, metric=\"cosine\")\n",
    "#     corr_dist = cdist(x_true, x_pred, metric=\"correlation\")\n",
    "#     avg_l2 = l2_dist.sum() / np.prod(l2_dist.shape)\n",
    "#     avg_cos = cos_dist.sum() / np.prod(cos_dist.shape)\n",
    "#     avg_corr = corr_dist.sum() / np.prod(corr_dist.shape)\n",
    "\n",
    "\n",
    "#     ot_solver = geomloss.SamplesLoss(\n",
    "#         \"sinkhorn\", p=2, blur=0.05, scaling=0.5, debias=True, backend=\"tensorized\"\n",
    "#     )\n",
    "#     ot = ot_solver(\n",
    "#         torch.tensor(x_pred).type(torch.float32).to(device),\n",
    "#         torch.tensor(x_true).type(torch.float32).to(device),\n",
    "#     ).item()\n",
    "#     # l2 = nn.MSELoss(x_pred, x_true)\n",
    "\n",
    "#     print(f'ot: {ot}, l2: {avg_l2}, cos: {avg_cos}, corr: {avg_corr}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DYffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
