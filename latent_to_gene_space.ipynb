{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import scanpy as sc\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "from torch.optim import lr_scheduler\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from umap.umap_ import UMAP\n",
    "# from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import cudf\n",
    "from cuml import PCA, UMAP\n",
    "import cupy as cp\n",
    "import pickle\n",
    "\n",
    "from model_VAE import VAE\n",
    "from dataloader_VAE import get_h5ad_data, get_dataloader, normalize, inverse_normalize\n",
    "from train_VAE import train_vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'MP'\n",
    "\n",
    "result_path = f'/home/hanyuji/Results/scDYff/interpolation_latent/{dataset_name}_result_dict_50_latent.pt'\n",
    "with open(result_path, 'rb') as f:\n",
    "    latent_data_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "\n",
    "# data_list = get_h5ad_data('drosophila_scNODE2_2000genes_2489cells_11tps.h5ad')\n",
    "data_list = get_h5ad_data('pancreatic_scNODE4_2000genes_9483cells_4tps.h5ad')\n",
    "\n",
    "norm_data_list, scalers = normalize(data_list)\n",
    "\n",
    "\n",
    "# 训练集和测试集对应的下标\n",
    "all_index = [0, 1, 2, 3]\n",
    "train_index = [0, 1, 2]\n",
    "test_index = [0,1]\n",
    "\n",
    "train_list = [norm_data_list[i] for i in train_index]\n",
    "test_list = [norm_data_list[i] for i in test_index]\n",
    "\n",
    "train_dataloader = get_dataloader(train_list, train_index, batch_size=64)\n",
    "test_dataloader = get_dataloader(test_list, test_index, batch_size=64)\n",
    "all_dataloader = get_dataloader(norm_data_list, all_index, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "device = \"cuda:0\"\n",
    "net = VAE().to(device)\n",
    "model_path = f'/home/hanyuji/Results/VAE_result/model_para/vae_model_0604_{dataset_name}_all.pt'\n",
    "net.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备存放输出\n",
    "latent_list_mix=[]\n",
    "latent_dict = {}  # 12个数组 [], [], [], [], [], [], [], [], [], [], [], []\n",
    "recon_dict = {}  # 12个数组\n",
    "\n",
    "# {4: [], 6: [], 8: []}\n",
    "# for index in test_index:\n",
    "for index in all_index:\n",
    "    \n",
    "    latent_dict[index] = []\n",
    "    recon_dict[index] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "# for (x, y) in test_dataloader:\n",
    "for (x, y) in all_dataloader:\n",
    "    \n",
    "    x = x.float().to(device)\n",
    "    z, mu, var = net.encoder(x)\n",
    "    recon = net.decoder(z)\n",
    "    \n",
    "    z_np = z.detach().cpu().numpy()\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    recon_np = recon.detach().cpu().numpy()\n",
    "        \n",
    "    for (z_i,y_i) in zip(z_np,y_np):\n",
    "        latent_dict[y_i].append(z_i)        \n",
    "    for (recon_i,y_i) in zip(recon_np,y_np):\n",
    "        recon_dict[y_i].append(recon_i)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_list = []\n",
    "recon_list = []\n",
    "label_list = []\n",
    "for index, arr in latent_dict.items():\n",
    "    latent_list.append(np.asarray(arr))\n",
    "    for _ in arr:\n",
    "        label_list.append(index)\n",
    "    \n",
    "for index, arr in recon_dict.items():\n",
    "    recon_list.append(np.asarray(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "\n",
    "# 保存这些数组到一个文件\n",
    "\n",
    "file_name = 'MP_latent_50.pkl'\n",
    "with open('/home/hanyuji/Results/VAE_result/data_latent/'+file_name, 'wb') as f:\n",
    "    pickle.dump(latent_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DYffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
